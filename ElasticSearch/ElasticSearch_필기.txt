[Elastic Search]

RDBMS : OracleDB, MySQL,...
NoSQL : MongoDB, Elastic Search, ..

-----------------------------------------------------------------------------------------

엘라스틱서치는 Java언어로 이루어진 오픈소스 형태의 정보 검색 라이브러리 기반의
Apache Lucene(아파치 루씬)을 기반으로 만들어진 검색 엔진
초창기에는 Java만 사용해야했지만, 추가적인 개발을 통해서 C, 파이썬 등
다른 프로그래밍 언어를 사용할 수 있게 되었음

검색 엔진 이란, 웹상에 존재하는 정보와 웹사이트를 검색하기 위한 프로그램
그 프로그램으로 검색 서비스를 제공하는 곳은 '검색 사이트' 이지만.
사실상 검색엔진이라는 용어와 같이 사용되고 있음
	ex) 네이버, 구글, 다음 등
엘라스틱 서치는 검색엔진이지만, NoSQL데이터를 저장해서 이것저것 검색하기 때문에
NoSQL의 특성 역시 가지고 있음

===========================================================

[ES 아키텍처 & 용어]
- es에서 사용하는 대부분의 개념은 대부분 RDBMS에도 있는 개념

<용어>
	elastic search	vs	OracleDB
	index			DataBase
	type			Table
	field			column (field)
	document		row (data 한줄)

-ES의 인덱스
	> 오라클DB의 데이터베이스와 같은 것
	> 데이터가 저장되는 공간
	> !== RDBMS의 index가 아님

[CRUD]
	RDBMS SQL	vs	ES HTTP Method
	SELECT			GET
	INSERT			PUT
	UPDATE / SELECT		POST
	DELETE			DELETE

1. 클러스터(cluster)
	- ES에서 가장 큰 시스템 단위를 의미
	- 최소 하나 이상의 노드로 이루어진 노드들의 집합

2. 노드(node)
	- ES를 이루는 하나의 단위 프로세스

-----------------------------------------------------------------------------------------

[장점]
	- 오픈소스 검색엔진
	- 전문 검색(검색 성능이 빠르다)
	- 정형화 되지 않은 데이터도 검색하는 것이 가능
	- 시각화(kibana) : 키바나와 연동하여 실시간 로그분석 및 시각화 가능
	- RESTFUL API >> 요청이나 응답에 JSON을 이용
	/ HTTP프로토콜을 통해서 데이터를 검색, 추가, 수정, 삭제할 수 있음
	- 인덱스가 서로 달라도 검색하는 필드명이 같으면
		여러 인덱스 한번에 조회가 가능
	- 확장성이 뛰어남 : 데이터의 양이 증가할때 서버의 확장도 용이함

============================================================

[단점]
	- 운영이 복잡함
	- 메모리의 사용량이 높음
	- 문서의 크기가 큰 경우 처리속도가 느려질 수 있음
	- 완전한 실시간성 보장 안함 : 100% 완전한 실시간은 아님
		>> 한번 색인된 데이터는 1초이상 후에 검색 가능

==============================================================

[Elastic Search 설치]
1. https://www.elastic.co/kr/elasticsearch
2. Elastic Search 다운로드 클릭
3. 화면 우측에 View past releases 클릭
4. Elasticsearch 7.12.0 Windows버전 다운로드
5. 압축풀기

6. bin폴더 - 실행파일, 플러그인 설치파일 등 실행관련 파일들이 있음
   config폴더 - 설정파일(elasticsearch.yml)등을 포함한 설정관련 파일이 있음
	그 외에는 데이터, 로그, 플러그인 등이 모여있는 폴더인데 신경 안써도 됨

[실행]
1. C:\Users\sdedu\Desktop\elasticsearch-7.12.0-windows-x86_64\elasticsearch-7.12.0\bin (bin 폴더)
	에서 elasticsearch.bat 실행
2. C:\Users\sdedu\Desktop\elasticsearch-7.12.0-windows-x86_64\elasticsearch-7.12.0 에서 
	cmd실행후
	curl -X GET "localhost:9200/?pretty" 명령어실행

	응답 결과를 JSON형태로 보내주는데, URL뒤에 >pretty를 추가하면 가독성이 좋은 형태로 결과를 제공해줌
	curl:
		다양한 통신 프로토콜을 지원하여 데이터를 전송할 수 있는 소프트웨어
		주로 사용되는 옵션은
			-X : 요청시 사용할 메소드를 정함 [GET/POST/PUT/DELETE]
			-d : 데이터를 전송할 수 있다. POST요청에만 사용
			-o : 받아온 데이터를 콘솔에 보여주는 것이 아닌 파일형태로 저장

=========================================================================

[키바나 설치]

키바나는 단순 데이터 시각화 툴로 오해하는 사람들이 있는데,
ES의 관리, 모니터링, 솔루션을 총괄해주는 메인 UI
ES에서 제공되는 시계열 데이터나 위치 분석 같은 다양한 분석결과를 시각화함
대부분의 기능들은 무료버전수준에서 제공해줌

1. https://www.elastic.co/kr/kibana 로 접속
2. Kibana 다운로드 클릭
3. 화면우측에 past~눌러서 7.12.0버전 찾아서 Windows버전 설치
4. 압축풀기

5. bin폴더 - 실행파일, 플러그인설치 같은 실행관련 파일들
   config폴더 - 설정파일(kibana.yml)과 기타 설정파일 등
	그외의 폴더는 신경 안써도됨

[실행]
1. C:\Users\sdedu\Desktop\kibana-7.12.0-windows-x86_64\bin 의 kibana.bat 실행
2. 정상적으로 실행되면 크롬에서 localhost:5601 주소로 요청
	***ES가 먼저 실행된 후 키바나를 실행해야함
3. 화면에 Add data와 Explore on my own 두가지 선택지
	Add data는 샘플데이터 불러오기
	Explore on my own은 홈으로 가기
4. Explore on my own 눌러서 홈으로 이동

=========================================================================

[ES Basic]
ES의 모든 요청과 응답은 REST API 형태로 제공됨
REST는 'Representational State Transfer'의 약자
	HTTP 프로토콜의 장점을 이용해 리소스들을 주고 받는 형태
REST API는 REST를 기반으로 API를 서비스 하는 것을 의미
REST API는 4가지 메소드타입(GET/POST/PUT/DELETE)를 가지고
	CRUD작업을 진행


[키바나 콘솔 사용법]
키바나 Dev Tools에 있는 콘솔을 이용해서 REST API를 호출할 것
키바나 홈페이지 상단의 Dev Tools 클릭

카바나 콘솔을 이용하면 복잡한 앱개발이나 프로그램 설치 없이
ES와 REST API로 통신할 수 있음
왼쪽 입력창에서 ES에서 제공하는 REST API를 입력하고
세모 모양 실행버튼 or ctrl+enter를 누르면 HTTP 요청을 하게되고,
오른쪽 출력창에서 HTTP의 응답을 확인할 수 있음
또한 키바나 콘솔은 ES API 자동완성 기능이 지원되기 때문에 외울필요 없음

===========================================================================

[시스템 상태 확인]
ES의 현재 상태를 빠르게 확인할 수 있는 방법으로 일반적으로
	cat API를 사용함
cat은 'compact and aligned text' 의 약자로,
	콘솔에서 시스템 상태를 확인할 때 가독성을 높일 목적으로 만들어진 API
	키바나 콘솔에 GET _cat 입력후 실행

===========================================================

도큐먼트는 반드시 하나의 인덱스에 포함되어야 함
	=> Oracle에서 Table안에 데이터가 있어야 하는 것처럼
도큐먼트의 CRUD동작을 하기 위해서는 반드시 인덱스가 있어야함

인덱스 생성
PUT index1
index1 이라는 이름의 인덱스를 생성하는 API
PUT은 생성이나 수정을 위한 HTTP메소드,
index1은 인덱스의 이름


인덱스 확인
GET index1
index1의 설정값 등을 확인할 수 있음


인덱스 삭제
DELETE index1
지금은 인덱스 내에 도큐먼트가 하나도 없지만
인덱스를 삭제하면 인덱스에 저장되어있던
도큐먼트들도 모두 삭제되니 주의


[Document CRUD]
1. 도큐먼트 생성
	ES에서 도큐먼트를 인덱스에 포함시키는 것을 인덱싱(indexing)이라고 함

PUT index2/_doc/1
{
 "name" : "kwak",
 "age" : 10,
 "gender" : "male"
}
로 요청을 하면 존재하지 않았던 index2라는
인덱스를 생성하면서 동시에 인덱스에
도큐먼트를 인덱싱함
index2는 인덱스 이름,
_doc은 엔드포인트 구분을 위한 예약어,
숫자 1은 인덱싱할 도큐먼트의 고유 아이디
도큐먼트에는 name, age, gender 라는 필드가 있고,
각 필드에는 값이 있음

index2 인덱스가 정상적으로 생성되었는지 확인
GET index2

mapping을 확인해보면,
age는 long타입, gender, name에는 text타입으로
필드가 지정된 것을 확인 할 수 있음

우리가 직접 데이터 타입을 지정하지 않아도,
ES는 필드와 값을 보고 자동으로 자료형을 지정해줌
	=> 다이나믹매핑(Dynamic Mapping)

index2인덱스에 country라는 이름의 필드가 추가된 도큐먼트를 인덱싱
PUT index2/_doc/2
{
 "name" : "kwak1",
 "country" : "korea"
}

2번 도큐먼트는 country필드는 추가되었고, 기존에 있던 age, gender필드는 사용하지 않았지만
문제없이 인덱싱 됨
	=> ES는 NoSQL의 특성을 갖고 있음

데이터타입을 잘못 입력한 도큐먼트를 인덱싱
PUT index2/_doc/3
{
 "name" : "kwak2",
 "age" : "50",
 "gender" : "female"
}

index2인덱스는 age를 실수(long)타입으로 매핑했는데
3번 도큐먼트는 텍스트 타입으로 입력함
RDBMS는 오류가 발생했겠지만
스키마에 유연하게 대처하는 ES는 타입을 변환해서 저장
ES는 혹시모를 사용자의 입력 실수를 고려해서
자동으로 데이터 형변환을 진행하는데 대표적으로
	- 숫자필드에 문자열이 입력되면 숫자로 변환을 시도
	   ex) "age" : "10" => 10
	- 정수필드에 실수가 입력되면 소수점 아래자리를 무시
	   ex) "age" : 10.0 => 10


도큐먼트 조회
도큐먼트 아이디를 사용해서 조회
GET index2/_doc/1

인덱스명과 도큐먼트의 아이디를 이용해
특정 도큐먼트의 데이터를 가져올 수 있음
GET index2/_search


3.도큐먼트 수정
PUT index2/_doc/1
{
 "name" : "deaver",
 "age": 99,
 "gender" : "female"
}
도큐먼트를 인덱싱 하는 과정에서
같은 도큐먼트의 아이디가 있으면 덮어쓰기가 되는건데
마치 업데이트 된것처럼 보임
REST API 응답결과를 보면 UPDATED라고 나옴

update API를 활용해서 업데이트도 가능함
POST index2/_update/1
{
 "doc":{
   "name":"feaver"
 }
}

_update라는 엔드포인트를 추가해서
특정 필드의 값만 업데이트도 가능함


4.도큐먼트 삭제
DELETE index2/_doc/3


==========================================================================

[매핑]
ES에서는 JSON형태의 데이터를 루씬이 이해할 수 있도록 바꿔주는 작업을 하는데, 이게 Mapping임
이 매핑을 ES가 자동으로 하면 '다이나믹 매핑'
사용자가 직접 설정하면 '명시적 매핑'

앞에서 index2 인덱스를 만들때 따로 매핑을 설정하지 않았음
하지만 도큐먼트가 인덱싱 되었던 이유는 다이나믹매핑때문인데, 특별히 자료형을 고민하지 않아도
JSON 도큐먼트의 자료형에 맞추서 ES 자동으로 매핑해줌
	
	자료형
	원본 데이터타입			다이나믹매핑으로 변환된 데이터타입
	null				필드추가 x
	boolean				boolean
	float				float
	integer				long
	object				object
	string				string데이터 형태에 따라서 date or text/keyword

예를 들어, JSON도큐먼트에 "age" : "20"이라는 필드와 값이 있다면, ES는 "20"을 숫자(integer)로 인식함
원본 코드가 integer 타입이기때문에 인덱스 생성시 age필드를 long타입 필드로 매핑함
일단 편리하기는 하지만, 숫자타입은 무조건 범위가 가장 넓은 long으로 매핑이 되어서
불필요한 메모리를 차지할 수 있으며, 문자열의 경우에는 검색, 정렬등을 고려한 매핑이 제대로 되지않음
Es는 인덱스 매핑값을 확인할 수 있는 mapping API를 제공함

GET index2/_mapping


2. 명시적 매핑
인덱스 매핑을 직접 정의하는것을 명시적매핑(Explict Mapping)이라고 함
인덱스를 생성할때 mappings 정의하거나, mapping API를 이용해 매핑을 지정할 수 있음

PUT index3
{
 "mappings":{
   "properties":{ 
     "age" : {"type": "short"},
     "name" : {"type" : "text"},
     "gender" : {"type" : "keyword"}
  }
 }
}

인덱스를 생성할때 mappings, properties 아래에
필드명과 필드 타입을 지정해주면 됨
index3라는 새로운 인덱스를 생성하면서 명시적으로 매핑을 지정하고 있는데
이를 위해서는 데이터 타입을 이해하고 있어야함


이를 위해서는 데이터타입을 이해하고 있어야함
[데이터타입]
텍스트 : text, keyword
날짜 : date
정수 : byte, short, integer, long
실수 : float, double
논리 : boolean
객체 : object
위치정보 : get-point(위도, 경도), geo-shape(지형)
배열 : nested

이 중 문자열타입이 기존 string에서 text와 keyword라는
두가지 타입으로 분리가 되었음

-text타입 : 일반적으로 문장을 저장하는 매핑타입
PUT text_index
{
 "mappings":{
  "properties":{
    "contents" : {"type" : "text"}
  }
 }
}

생성후 도큐먼트 인덱싱
PUT text_index/_doc/1
{
 "contents" : "beautiful day"
}

쿼리 불러우기
GET text_index/_search
{
 "query":{
  "match":{
   "contents" : "day"
  }
 }
}


match는 전문 검색을 할 수 있는 쿼리이고
contents 필드에 있는 용어 중 일치하는 용어가 있는
도큐먼트를 찾는 쿼리문
contents를 "beautiful day", "beautiful" 등으로 바꿔서
요청을 해도 그 도큐먼트를 찾을 것


-keyword타입
키워드 타입은 카테고리나 사람이름, 브랜드등
규칙성이 있거나 유의미한 값들의 집합,
즉 범주형 데이터에 주로 사용

PUT keyword_index
{
 "mappings":{
  "properties":{
   "contents" : {"type" : "keyword"}
  }
 }
}

생성후 도큐먼트 인덱싱
PUT keyword_index/_doc/1
{
 "contents" : "beautiful day"
}


쿼리불러오기
GET keyword_index/_search
{
 "query":{
  "match":{
   "contents" : "beautiful"
  }
 }
}

쿼리의 결과 도큐먼트를 하나도 찾지못함
text_index와 달리 keyword_index는
정확하게 beautiful day 라고 입력해야 도큐먼트를 찾음
키워드 타입은 문자열 전체를 하나의 용어로 보고 인덱싱을 하기 때문에
텍스트가 정확하게 일치하는 경우에만 값을 검색함


==================================================================

범위 쿼리(날짜, 범위)
특정 날짜나 숫자의 범위를 지정해서 범위 안에 포함된 데이터들을 검색할 때 사용
날짜 / 숫자 타입의 데이터는 범위쿼리가 가능, text/kwayword 타입의 데이터에는 사용할 수 없음

[샘플 데이터 가져오기]
1. 키바나 홈 화면 이동
2. 우측 상단에 Add Data 클릭
3. Sample data 항목 고르기
4. Sample flight ndata - Add data 버튼 클릭
5. Home 화면 이동 - 콘솔로 이동


쿼리요청
GET kibana_sample_data_flights/_search
{
 "query" : {
  "range" : {
   "timestamp" :{
    "gte" : "2024-02-08",
    "lt" : "2024-02-09"
   }
  }
 }
}

timestamp 필드는 yyyy-mm-dd 형식의 포맷을 사용하는데
날짜 포맷 형태를 다르게 넣으면
parse_exception 오류 발생함(ex] 2024/02/08)
검색 범위를 지정하는 파라미터로는
gte(이상) / gt(초과) / lte(이하) / lt(미만)

===================================================

[키바나 맵스]
키바나는 ES에서 제공되는 시계열 데이터나 위치분석 같은 다양한 분석결과를 시각화 가능
그 중에서도 위치 기반 데이터를 지도 위에 표현할 수 있는 Maps 시각화 기능을 볼 것
키바나 맵스를 통해서 사용자는 위치정보가 포함된 데이터를 지도에 올려서 시각화 할 수 있고,
멀티레이어 기능을 통해 다양한 형태의 지도를 화면에서 볼 수 있음

[윈도우 웹서버 구축]
레이어를 추가하기 앞서 사용자가 만든 GeoJSON 파일을 가져오려면 웹서버가 필요함
엔진엑스 라는 것을 설치
	설치가 쉽고 빨라서 많이 이용되는 웹서버

1. nginx.org 접속
2. 화면우측의 downloads 클릭
3. Stable version의
4. nginx/Windows-1.24.0 다운 및 압축풀기

conf폴더의 nginx.conf 파일 수정
location 블록의 [root html;]의 다음줄에
	add_header Content-Security-Policy "default-src 'self';";
	add_header 'Access-Control-Allow-Origin' 'http://localhost:5601';
두줄 추가후 저장

첫번째줄 CSP(Content-Security-Policy) 옵션
	웹 보안 정책 중 하나로 각종 악성 스크립트를 막기 위해 사용

두번째줄 Access-Control-Allow-Origin은
스프링때도 봤듯이 요청을 보낸주소(키바나서버)와 받는주소(엔진엑스)가 다를 경우
CORS(Cross Origin Resource Sharing)오류가 발생함
	이것을 해결하고자 서버 응답 헤더에 프론트 주소를 적어준것

설정 마쳤으면 exe파일 실행
브라우저에서 http://localhost를 입력하면 실행화면이 나옴

cf) 종료하고 싶다면
C:\Users\sdedu\Desktop\nginx-1.24.0 에서 cmd실행 후 
	nginx.exe -s stop 실행

웹 서버가 구축 되었다면 사용자 지정 타일맵과 GeoJSON을 적용해보자
키바나는 위치/지역 정보를 처리할 수 있는데, 위치는 단순 좌표이고, 
지역은 위치가 모여서 만드는 폴리곤(polygon, 다각형)으로 키바나에서는 벡터레이어 라고 함

엘라스틱 서치가 제공하는 벡터레이어는
	https://maps.elastic.co 사이트에서 확인 가능

전세계 행정구역 정보를 벡터레이어라는 폴리곤 단위로 제공하는데,
한국에 대해서는 시도, 시군구 단위의 폴리곤을 제공함
작업을 하다보면 행정동 단위 / 우편번호단위 등 다양한 형태의 벡터 레이어가 필요할 수 있음
서울시 우편번호 폴리곤이 담겨있는 GeoJSON파일을 벡터레이어로 사용

C:\Users\sdedu\Desktop\nginx-1.24.0 여기에 있는 html폴더에 json파일 넣기
html폴더는 nginx에서 root가 되는 웹사이트 경로

그 다음에는 키바나 설정파일(kibana.yml)의 수정
	파일 제일 마지막에 아래 내용을 추가후 저장

map.regionmap:
    layers:
      -name:"SEOUL ZIPCODE"
       url: "http://localhost/TL_KODIS_BAS_11.geojson"
       attribution: "INRAP"
       fields:
	-name: "BAS_MGT_SN"
	 description: "zipcode number"

map.regionmap은 사용자 벡터 레이어를 서정
layers밑에 레이어 이름을 정하고 URL을 적어주는데
**키바나 서버와 도메인의 CORS가 가능해야 함
attrbution은 getJSON 파일을 참고하는 방법을 정의한 것
fields는 geoJSON 속성 중 노출할 필드를 정의
그 중 BAS_MGT_SN이라는 필드는 우편번호를 나타내는 값

참고로 이 yml파일은 들여쓰기에 신경써줘야하는데
https://www.yamllint.com에서 입력 오류가 없는지 확인


설정파일이 변경되었으니 키바나 껐다가 다시 실행

실행완료후 키바나 맵스에서 Add layer를 클릭해보면
Configured GeoJSON이라는 메뉴가 생겻을것
방금 설정한 우편번호 레이어 목록이 나오는지 확인

=================================================================

클러스터와 그리드
2개의 레이어를 만들건데
하나는 클러스터형태, 하나는 그리드형태

- 클러스터
1. map에서 create map으로 가서
	add layer 누르고 clusters and grids를 선택
2. index pattern은 kibana_sample_data_flights를 선택
3.Geospatial field 는 DestLocation
4. Show as는 clusters선택
5.Add layer

6.Name은 cluster
7.Metrics에서 카운트 최소 최대 평균같은
	집계를 구할 수 있는데 count선택
8. Grid parameters 에서는 정확도를 설정할수 잇음
	coarse값에서 super fine으로 바꿔보면
	cluster위치 정확도가 더 정교해질 것
9. save & close버튼 눌러서 저장

-레이어
1. map에서 create map으로 가서
	add layer 누르고 clusters and grids를 선택
2. index pattern은 kibana_sample_data_flights를 선택
3.Geospatial field 는 DestLocation
4. Show as는 grid선택
5. Add layer

6. 이름 grid
7. grid parameters - show as - grids


2개의 레이어가 생겼고
grid레이어는 사각형 격자로 보이고
cluster레이어는 원형형태로 보임
지도를 확대해보면 하나의 큰 클러스터로 잡혔던 내용들이 세부적으로 나뉘는것을 볼 수 있음

GeoJSON 적용하기
EMS(Elastic Maps Service)에서 제공하는 벡터레이어 이외에
사용자가 만든 벡터 레이어 등록이 필요할때 사용하는 기능
GeoJSON 형태의 파일만 업로드가 가능
GeoJSON은 위치 정보를 JSON형태로 표현한 표준 포맷
점 선 다각형 등을 지형지물이랑 연계해 위치를 표시하는 용도로 사용
타입과 좌표를 이용해서 영역을 표시하고 프로퍼티에 설명을 적을 수 있음

Add layer 선택해서 Configured GeoJSON선택
아까 넣어놓은 벡터레이어(SEOUL ZIPCODE) 불러와서
Add layer버튼 누르면 상세설정 가능

서울지역의 우편번호 경계선을 표현하는 GeoJSON파일
이 벡터레이어와 조인할 도큐먼트가 필요한데, 현재 샘플데이터에는 서울 우편번호를 가지고 있는
	도큐먼트가 따로 없기 때문에 간단한 인덱스 만들 것

 -새로운 크롬창에서 kibana접속 -> 콘솔열기
PUT kibana_ems_index/_doc/1
{
 "BAS_MGT_SN" : "1156000102"
}
PUT kibana_ems_index/_doc/2
{
 "BAS_MGT_SN" : "1156000100"
}
PUT kibana_ems_index/_doc/3
{
 "BAS_MGT_SN" : "1156000044"
}

GET kibana_ems_index/_search 로 확인까지


키바나에서 사용할 수 있는 인덱스 패턴 지정

키바나 홈 => Manage - 화면좌측 Index Patterns - Create index pattern 버튼 - 패턴이름 kibana_ems_index로 지정
 - Next step 버튼 - Create index pattern 버튼 - 완료했으면 Map으로 이동

Map으로 돌아와서 Term joins의 Add join 클릭

Left는 .geojson 파일의 zipcode number 필드로,
키바나 설정파일의 map.regionmap.layers.fields에서 지정한 이름
Right는 만들었던 kibana_ems_index의 BAS_MGT_SN.keyword 필드

Save & close 눌러서 조인을 하면
kibana_ems_index의 3개의 도큐먼트만 보이고, 나머지 폴리곤은 count가 0이되어 사라지게 됨

사용자가 원하는 벡터레이어를 추가하고 필요에 맞게 사용할 수 있음

===============================================================================

PUT seoul_wifi
{
 "settings" : {
  "analysis" : {
   "analyzer" : {
    "korean" : {
     "tokenizer" : "nori_tokenizer"
    }
   }
  }
 },
 "mappings" : {
  "properties" : {
   "gu_nm" : {"type" : "keyword"},
   "place_nm" : {"type" : "text", "analyzer" : "korean"},
   "instl_xy" : {"type" : "geo_point"}
  }
 }
}

이후 GET seoul_wifi/_search로 확인

데이터가 잘 들어갔으면 시각화 ㄱㄱ
키바나에서 seoul_wifi 라는 인덱스패턴을 만들기

키바나홈 -> Manage - 화면 좌측 index Patterns - Create index pattern - 이름은 seoul_wifi
-> Next step - Create index Pattern버튼

Map 열어서 서울지도 열고
	Add layer로 Documents 선택 후 seoul_wifi 선택
	-scaling 은 Show cluster~~~ 고르기
	-Add layer버튼 누르고
	- Filtering 쪽에서 (input)