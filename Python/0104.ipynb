{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4bba332-a6a6-4e89-a705-47aa4fa1b64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "activation\n",
    "활성화 함수 : linear(선형회귀) : 연속적인 값에 대한 결과를 가져올때\n",
    "              sigmoid(참/거짓 : 이항) : 렐루(ReLU),\n",
    "                                 하이퍼볼릭 탄젠트(hyperbolic tangent)\n",
    "                                 소프트 플러스(softplus)\n",
    "              softmax(다항분류) : 출력결과가 여러개인 경우: 0.2 + 0.6 + 0.3 = 1 : 최고값이 답\n",
    "\n",
    "loss\n",
    "손실함수 :  평균제곱오차(mse) : mean_squared_error\n",
    "            평균 절대오차(mae)\n",
    "            평균절대백분율오차(mape)\n",
    "            평균제곱로그오차(msle)\n",
    "            이형교차엔트로피(binary_crossentropy) : 이항분류\n",
    "            범주형 교차 엔트로피(calegorical_crossentropy) : 다항분류\n",
    "\n",
    "optimizer\n",
    "오차수정함수 : 경하강법(gd) : 확률적경사하강법(sgd) : 아다그라이드(adagrad) : 알엠에스프롭(RMSProp): 아담(adam)\n",
    "                                                      모멘트(momentum)                            : 아담(adam)\n",
    "                                                                                                  : 네스레포트 모멘텀(NAG)\n",
    "\n",
    "모델수행의 결과 출력(metrics) : \n",
    "    accuracy : 학습셋에 대한 정확도에 기반해 결과를 출력\n",
    "    loss : 학습셋에 대한 손실값을 나타냄\n",
    "    val_acc : 테스트셋에 대한 정확도를 나타냄\n",
    "    val_loss : 테스트셋에 대한 손실값을 나타냄\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df40435-9e82-4009-87e3-bd6c05ba9ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/4e/ba/ce9bd1cd4953336a0e213b29cb80bb11816f2a93de8c99f88ef0b446ad0c/scikit_learn-1.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/43/d0/f3cd75b62e1b90f48dbf091261b2fc7ceec14a700e308c50f6a69c83d337/scipy-1.11.4-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.11.4-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/60.4 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.4 kB ? eta -:--:--\n",
      "     -------------------------------- ----- 51.2/60.4 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.4/60.4 kB 533.6 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "   ---------------------------------------- 0.0/9.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/9.2 MB 13.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.0/9.2 MB 63.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 84.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 84.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.2/9.2 MB 45.3 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.4-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "   ---------------------------------------- 0.0/44.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.7/44.1 MB 81.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 8.6/44.1 MB 92.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 13.4/44.1 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 17.6/44.1 MB 110.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 21.9/44.1 MB 93.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.0/44.1 MB 93.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 30.3/44.1 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.9/44.1 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 39.7/44.1 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.1/44.1 MB 32.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.11.4 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b33cde99-a25d-4f01-bf83-177d2b5db86d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "28/28 [==============================] - 1s 8ms/step - loss: 38681354240.0000 - val_loss: 39292973056.0000\n",
      "Epoch 2/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 37748797440.0000 - val_loss: 37898735616.0000\n",
      "Epoch 3/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 35723591680.0000 - val_loss: 34773004288.0000\n",
      "Epoch 4/2000\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 25941972992.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 3ms/step - loss: 31470313472.0000 - val_loss: 28650227712.0000\n",
      "Epoch 5/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 23791001600.0000 - val_loss: 18730911744.0000\n",
      "Epoch 6/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 13099912192.0000 - val_loss: 7671707648.0000\n",
      "Epoch 7/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 4476929536.0000 - val_loss: 2441870592.0000\n",
      "Epoch 8/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2136687872.0000 - val_loss: 2107801600.0000\n",
      "Epoch 9/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2072509696.0000 - val_loss: 2093019392.0000\n",
      "Epoch 10/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2064270464.0000 - val_loss: 2085547392.0000\n",
      "Epoch 11/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2057704064.0000 - val_loss: 2080991744.0000\n",
      "Epoch 12/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2057328000.0000 - val_loss: 2075416960.0000\n",
      "Epoch 13/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2050605568.0000 - val_loss: 2071671552.0000\n",
      "Epoch 14/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2049205632.0000 - val_loss: 2065010816.0000\n",
      "Epoch 15/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2048085760.0000 - val_loss: 2059458432.0000\n",
      "Epoch 16/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2040764544.0000 - val_loss: 2058832256.0000\n",
      "Epoch 17/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2040946176.0000 - val_loss: 2050544000.0000\n",
      "Epoch 18/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2033658112.0000 - val_loss: 2045166720.0000\n",
      "Epoch 19/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2029860480.0000 - val_loss: 2040912512.0000\n",
      "Epoch 20/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2031435264.0000 - val_loss: 2037975040.0000\n",
      "Epoch 21/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2036543488.0000 - val_loss: 2030033280.0000\n",
      "Epoch 22/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2018494592.0000 - val_loss: 2027307904.0000\n",
      "Epoch 23/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2017735296.0000 - val_loss: 2020231424.0000\n",
      "Epoch 24/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2014508160.0000 - val_loss: 2015536768.0000\n",
      "Epoch 25/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2008412800.0000 - val_loss: 2010716288.0000\n",
      "Epoch 26/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2010289024.0000 - val_loss: 2006193408.0000\n",
      "Epoch 27/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2003724544.0000 - val_loss: 2004681600.0000\n",
      "Epoch 28/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2004904064.0000 - val_loss: 1997667200.0000\n",
      "Epoch 29/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2002138112.0000 - val_loss: 1993256576.0000\n",
      "Epoch 30/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1994347904.0000 - val_loss: 1990781952.0000\n",
      "Epoch 31/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2007737216.0000 - val_loss: 1984530816.0000\n",
      "Epoch 32/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1998338176.0000 - val_loss: 1980896768.0000\n",
      "Epoch 33/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1995170560.0000 - val_loss: 1976964608.0000\n",
      "Epoch 34/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1989681536.0000 - val_loss: 1972409216.0000\n",
      "Epoch 35/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1990871936.0000 - val_loss: 1968359168.0000\n",
      "Epoch 36/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1983605376.0000 - val_loss: 1964973568.0000\n",
      "Epoch 37/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1987163904.0000 - val_loss: 1963888768.0000\n",
      "Epoch 38/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1985257472.0000 - val_loss: 1960743296.0000\n",
      "Epoch 39/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1973693440.0000 - val_loss: 1955939712.0000\n",
      "Epoch 40/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1975881088.0000 - val_loss: 1953165312.0000\n",
      "Epoch 41/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1979547136.0000 - val_loss: 1949141248.0000\n",
      "Epoch 42/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1969727232.0000 - val_loss: 1945022976.0000\n",
      "Epoch 43/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1965313024.0000 - val_loss: 1942215296.0000\n",
      "Epoch 44/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1966840064.0000 - val_loss: 1939335296.0000\n",
      "Epoch 45/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1960837376.0000 - val_loss: 1940397952.0000\n",
      "Epoch 46/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1960547328.0000 - val_loss: 1935994752.0000\n",
      "Epoch 47/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1957547136.0000 - val_loss: 1935530112.0000\n",
      "Epoch 48/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1959986560.0000 - val_loss: 1927352704.0000\n",
      "Epoch 49/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1959190656.0000 - val_loss: 1932102912.0000\n",
      "Epoch 50/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1958308608.0000 - val_loss: 1924269568.0000\n",
      "Epoch 51/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1954369536.0000 - val_loss: 1925786112.0000\n",
      "Epoch 52/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1965389824.0000 - val_loss: 1923586176.0000\n",
      "Epoch 53/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1951552640.0000 - val_loss: 1915800448.0000\n",
      "Epoch 54/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1952176000.0000 - val_loss: 1912812672.0000\n",
      "Epoch 55/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1944313728.0000 - val_loss: 1910998400.0000\n",
      "Epoch 56/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1945805824.0000 - val_loss: 1907920128.0000\n",
      "Epoch 57/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1942275072.0000 - val_loss: 1906038912.0000\n",
      "Epoch 58/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1947335680.0000 - val_loss: 1909414272.0000\n",
      "Epoch 59/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1941517568.0000 - val_loss: 1901150208.0000\n",
      "Epoch 60/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1945864192.0000 - val_loss: 1899054848.0000\n",
      "Epoch 61/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1938532608.0000 - val_loss: 1896886528.0000\n",
      "Epoch 62/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1937737088.0000 - val_loss: 1895308288.0000\n",
      "Epoch 63/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1949524864.0000 - val_loss: 1898790400.0000\n",
      "Epoch 64/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1936249472.0000 - val_loss: 1891032576.0000\n",
      "Epoch 65/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1934629760.0000 - val_loss: 1889884800.0000\n",
      "Epoch 66/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1940874624.0000 - val_loss: 1890918784.0000\n",
      "Epoch 67/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1932449280.0000 - val_loss: 1885562496.0000\n",
      "Epoch 68/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1931556352.0000 - val_loss: 1883796608.0000\n",
      "Epoch 69/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1938098176.0000 - val_loss: 1885559424.0000\n",
      "Epoch 70/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1929617280.0000 - val_loss: 1882929792.0000\n",
      "Epoch 71/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1925349504.0000 - val_loss: 1879265024.0000\n",
      "Epoch 72/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1953646464.0000 - val_loss: 1902524032.0000\n",
      "Epoch 73/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1933960192.0000 - val_loss: 1877185408.0000\n",
      "Epoch 74/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1938750848.0000 - val_loss: 1878300288.0000\n",
      "Epoch 75/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1948649216.0000 - val_loss: 1875203712.0000\n",
      "Epoch 76/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1930009856.0000 - val_loss: 1879881472.0000\n",
      "Epoch 77/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1925809152.0000 - val_loss: 1872241408.0000\n",
      "Epoch 78/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1935087104.0000 - val_loss: 1880122368.0000\n",
      "Epoch 79/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1921709568.0000 - val_loss: 1867937024.0000\n",
      "Epoch 80/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1923025792.0000 - val_loss: 1866535936.0000\n",
      "Epoch 81/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1923355136.0000 - val_loss: 1864920960.0000\n",
      "Epoch 82/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1927860224.0000 - val_loss: 1863660800.0000\n",
      "Epoch 83/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1926707840.0000 - val_loss: 1866523648.0000\n",
      "Epoch 84/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1916384768.0000 - val_loss: 1861266048.0000\n",
      "Epoch 85/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1916300672.0000 - val_loss: 1860247040.0000\n",
      "Epoch 86/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1922396416.0000 - val_loss: 1859595776.0000\n",
      "Epoch 87/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1913723520.0000 - val_loss: 1857568256.0000\n",
      "Epoch 88/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1916124928.0000 - val_loss: 1857569280.0000\n",
      "Epoch 89/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1919527808.0000 - val_loss: 1855988736.0000\n",
      "Epoch 90/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1913894016.0000 - val_loss: 1856592768.0000\n",
      "Epoch 91/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1921569664.0000 - val_loss: 1853122816.0000\n",
      "Epoch 92/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1920465280.0000 - val_loss: 1853982080.0000\n",
      "Epoch 93/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1916683264.0000 - val_loss: 1851145472.0000\n",
      "Epoch 94/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1915767680.0000 - val_loss: 1852012160.0000\n",
      "Epoch 95/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1910071552.0000 - val_loss: 1849078528.0000\n",
      "Epoch 96/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1915229056.0000 - val_loss: 1849099648.0000\n",
      "Epoch 97/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1912731008.0000 - val_loss: 1850391040.0000\n",
      "Epoch 98/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1917911936.0000 - val_loss: 1846241024.0000\n",
      "Epoch 99/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1930558336.0000 - val_loss: 1855124480.0000\n",
      "Epoch 100/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1916253184.0000 - val_loss: 1847661824.0000\n",
      "Epoch 101/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1913732480.0000 - val_loss: 1845277568.0000\n",
      "Epoch 102/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1916933504.0000 - val_loss: 1843257216.0000\n",
      "Epoch 103/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1921049984.0000 - val_loss: 1852113536.0000\n",
      "Epoch 104/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1920885632.0000 - val_loss: 1842597120.0000\n",
      "Epoch 105/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1915770752.0000 - val_loss: 1847710208.0000\n",
      "Epoch 106/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1913659520.0000 - val_loss: 1840752512.0000\n",
      "Epoch 107/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1916323328.0000 - val_loss: 1839864320.0000\n",
      "Epoch 108/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1919582208.0000 - val_loss: 1854498176.0000\n",
      "Epoch 109/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1913934848.0000 - val_loss: 1838076800.0000\n",
      "Epoch 110/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1912453120.0000 - val_loss: 1838602624.0000\n",
      "Epoch 111/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1913006336.0000 - val_loss: 1837117312.0000\n",
      "Epoch 112/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1908761344.0000 - val_loss: 1836112768.0000\n",
      "Epoch 113/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1919607936.0000 - val_loss: 1848035584.0000\n",
      "Epoch 114/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1914773632.0000 - val_loss: 1840471296.0000\n",
      "Epoch 115/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1907493760.0000 - val_loss: 1834145920.0000\n",
      "Epoch 116/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1908449920.0000 - val_loss: 1839405312.0000\n",
      "Epoch 117/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1908028416.0000 - val_loss: 1833261056.0000\n",
      "Epoch 118/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1903678592.0000 - val_loss: 1832652160.0000\n",
      "Epoch 119/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1906160384.0000 - val_loss: 1832154240.0000\n",
      "Epoch 120/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1918214912.0000 - val_loss: 1839763200.0000\n",
      "Epoch 121/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905865856.0000 - val_loss: 1832978944.0000\n",
      "Epoch 122/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1916517760.0000 - val_loss: 1831359232.0000\n",
      "Epoch 123/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1914204544.0000 - val_loss: 1844700544.0000\n",
      "Epoch 124/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1909277824.0000 - val_loss: 1831543168.0000\n",
      "Epoch 125/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1909582464.0000 - val_loss: 1832396160.0000\n",
      "Epoch 126/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1914530944.0000 - val_loss: 1850642432.0000\n",
      "Epoch 127/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1908647808.0000 - val_loss: 1835302400.0000\n",
      "Epoch 128/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902705920.0000 - val_loss: 1830256768.0000\n",
      "Epoch 129/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905986816.0000 - val_loss: 1833989760.0000\n",
      "Epoch 130/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902380416.0000 - val_loss: 1826851584.0000\n",
      "Epoch 131/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1907815168.0000 - val_loss: 1826974080.0000\n",
      "Epoch 132/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1901238528.0000 - val_loss: 1826389504.0000\n",
      "Epoch 133/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903726848.0000 - val_loss: 1827725824.0000\n",
      "Epoch 134/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1920604800.0000 - val_loss: 1842624256.0000\n",
      "Epoch 135/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1926049536.0000 - val_loss: 1850855168.0000\n",
      "Epoch 136/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1917785600.0000 - val_loss: 1826201088.0000\n",
      "Epoch 137/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1914418432.0000 - val_loss: 1827603968.0000\n",
      "Epoch 138/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1904508672.0000 - val_loss: 1824243584.0000\n",
      "Epoch 139/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1909539456.0000 - val_loss: 1833386880.0000\n",
      "Epoch 140/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1900734080.0000 - val_loss: 1822918528.0000\n",
      "Epoch 141/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902626304.0000 - val_loss: 1822826624.0000\n",
      "Epoch 142/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898384000.0000 - val_loss: 1824695424.0000\n",
      "Epoch 143/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904798336.0000 - val_loss: 1828775936.0000\n",
      "Epoch 144/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1908704128.0000 - val_loss: 1822061696.0000\n",
      "Epoch 145/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1904411264.0000 - val_loss: 1821697280.0000\n",
      "Epoch 146/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906814976.0000 - val_loss: 1828336512.0000\n",
      "Epoch 147/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1912565632.0000 - val_loss: 1821263872.0000\n",
      "Epoch 148/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1898535424.0000 - val_loss: 1820806656.0000\n",
      "Epoch 149/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1903800448.0000 - val_loss: 1820686976.0000\n",
      "Epoch 150/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1899282944.0000 - val_loss: 1820298112.0000\n",
      "Epoch 151/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1907565184.0000 - val_loss: 1826533120.0000\n",
      "Epoch 152/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1910768640.0000 - val_loss: 1820239872.0000\n",
      "Epoch 153/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1907770880.0000 - val_loss: 1819264768.0000\n",
      "Epoch 154/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1925250560.0000 - val_loss: 1820752512.0000\n",
      "Epoch 155/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897440256.0000 - val_loss: 1820063104.0000\n",
      "Epoch 156/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904376064.0000 - val_loss: 1820482560.0000\n",
      "Epoch 157/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904432000.0000 - val_loss: 1822648960.0000\n",
      "Epoch 158/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902702336.0000 - val_loss: 1820350336.0000\n",
      "Epoch 159/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1908751360.0000 - val_loss: 1818014592.0000\n",
      "Epoch 160/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898486144.0000 - val_loss: 1820879360.0000\n",
      "Epoch 161/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902856704.0000 - val_loss: 1819547648.0000\n",
      "Epoch 162/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1910806400.0000 - val_loss: 1831309440.0000\n",
      "Epoch 163/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902068096.0000 - val_loss: 1817877504.0000\n",
      "Epoch 164/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1921189248.0000 - val_loss: 1828863488.0000\n",
      "Epoch 165/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1896273024.0000 - val_loss: 1817419520.0000\n",
      "Epoch 166/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906406272.0000 - val_loss: 1821293952.0000\n",
      "Epoch 167/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903894528.0000 - val_loss: 1818618752.0000\n",
      "Epoch 168/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899492992.0000 - val_loss: 1819989120.0000\n",
      "Epoch 169/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906322304.0000 - val_loss: 1825798272.0000\n",
      "Epoch 170/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1922366080.0000 - val_loss: 1824303360.0000\n",
      "Epoch 171/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906450560.0000 - val_loss: 1817997312.0000\n",
      "Epoch 172/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1908003584.0000 - val_loss: 1827291776.0000\n",
      "Epoch 173/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1912786048.0000 - val_loss: 1820023040.0000\n",
      "Epoch 174/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905106816.0000 - val_loss: 1827411584.0000\n",
      "Epoch 175/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1904529536.0000 - val_loss: 1815780864.0000\n",
      "Epoch 176/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901076096.0000 - val_loss: 1816156800.0000\n",
      "Epoch 177/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1896914560.0000 - val_loss: 1815131776.0000\n",
      "Epoch 178/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897510912.0000 - val_loss: 1815864192.0000\n",
      "Epoch 179/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904172544.0000 - val_loss: 1815815552.0000\n",
      "Epoch 180/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1912583808.0000 - val_loss: 1814117888.0000\n",
      "Epoch 181/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1910324992.0000 - val_loss: 1825305344.0000\n",
      "Epoch 182/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1900183680.0000 - val_loss: 1813834880.0000\n",
      "Epoch 183/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900641536.0000 - val_loss: 1813870720.0000\n",
      "Epoch 184/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1938918528.0000 - val_loss: 1814245504.0000\n",
      "Epoch 185/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1909672704.0000 - val_loss: 1813825152.0000\n",
      "Epoch 186/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898840576.0000 - val_loss: 1815075712.0000\n",
      "Epoch 187/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906695296.0000 - val_loss: 1815044352.0000\n",
      "Epoch 188/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1927907328.0000 - val_loss: 1815129728.0000\n",
      "Epoch 189/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1917775360.0000 - val_loss: 1812873856.0000\n",
      "Epoch 190/2000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1899703168.0000 - val_loss: 1812483072.0000\n",
      "Epoch 191/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1906459392.0000 - val_loss: 1812312320.0000\n",
      "Epoch 192/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900278656.0000 - val_loss: 1812618496.0000\n",
      "Epoch 193/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901647360.0000 - val_loss: 1812471168.0000\n",
      "Epoch 194/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1903407872.0000 - val_loss: 1812182272.0000\n",
      "Epoch 195/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903076608.0000 - val_loss: 1818280704.0000\n",
      "Epoch 196/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901885824.0000 - val_loss: 1815500672.0000\n",
      "Epoch 197/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1922238976.0000 - val_loss: 1828501632.0000\n",
      "Epoch 198/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899497856.0000 - val_loss: 1830864896.0000\n",
      "Epoch 199/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904220416.0000 - val_loss: 1812238976.0000\n",
      "Epoch 200/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899102336.0000 - val_loss: 1816311296.0000\n",
      "Epoch 201/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896012288.0000 - val_loss: 1816337152.0000\n",
      "Epoch 202/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1904331520.0000 - val_loss: 1811786112.0000\n",
      "Epoch 203/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1913643136.0000 - val_loss: 1811864704.0000\n",
      "Epoch 204/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1901730944.0000 - val_loss: 1810985856.0000\n",
      "Epoch 205/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896112000.0000 - val_loss: 1811280768.0000\n",
      "Epoch 206/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895180032.0000 - val_loss: 1815410944.0000\n",
      "Epoch 207/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1919296128.0000 - val_loss: 1815133952.0000\n",
      "Epoch 208/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897979776.0000 - val_loss: 1812552960.0000\n",
      "Epoch 209/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1899207296.0000 - val_loss: 1810423680.0000\n",
      "Epoch 210/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1926780416.0000 - val_loss: 1829800064.0000\n",
      "Epoch 211/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899558528.0000 - val_loss: 1810506752.0000\n",
      "Epoch 212/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896007680.0000 - val_loss: 1810512768.0000\n",
      "Epoch 213/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1907815296.0000 - val_loss: 1809856384.0000\n",
      "Epoch 214/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1907787392.0000 - val_loss: 1816003840.0000\n",
      "Epoch 215/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902942848.0000 - val_loss: 1809765632.0000\n",
      "Epoch 216/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1906317568.0000 - val_loss: 1809476736.0000\n",
      "Epoch 217/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905368704.0000 - val_loss: 1809530752.0000\n",
      "Epoch 218/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900386944.0000 - val_loss: 1810238976.0000\n",
      "Epoch 219/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902097664.0000 - val_loss: 1809794432.0000\n",
      "Epoch 220/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1913003008.0000 - val_loss: 1813937664.0000\n",
      "Epoch 221/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903510144.0000 - val_loss: 1818621952.0000\n",
      "Epoch 222/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901880704.0000 - val_loss: 1819982720.0000\n",
      "Epoch 223/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1930341760.0000 - val_loss: 1809149952.0000\n",
      "Epoch 224/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891531648.0000 - val_loss: 1818775040.0000\n",
      "Epoch 225/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904905344.0000 - val_loss: 1810838528.0000\n",
      "Epoch 226/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905697536.0000 - val_loss: 1820642944.0000\n",
      "Epoch 227/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1896481536.0000 - val_loss: 1808383488.0000\n",
      "Epoch 228/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891866880.0000 - val_loss: 1813287040.0000\n",
      "Epoch 229/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896275456.0000 - val_loss: 1809061632.0000\n",
      "Epoch 230/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902615296.0000 - val_loss: 1822837888.0000\n",
      "Epoch 231/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1921899776.0000 - val_loss: 1832146176.0000\n",
      "Epoch 232/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900898432.0000 - val_loss: 1831065984.0000\n",
      "Epoch 233/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1907946240.0000 - val_loss: 1808254976.0000\n",
      "Epoch 234/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895892352.0000 - val_loss: 1810721280.0000\n",
      "Epoch 235/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1915055616.0000 - val_loss: 1818468224.0000\n",
      "Epoch 236/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1907467904.0000 - val_loss: 1807960960.0000\n",
      "Epoch 237/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899115392.0000 - val_loss: 1808511232.0000\n",
      "Epoch 238/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897495680.0000 - val_loss: 1808722944.0000\n",
      "Epoch 239/2000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1895342464.0000 - val_loss: 1807924736.0000\n",
      "Epoch 240/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1914359936.0000 - val_loss: 1829415936.0000\n",
      "Epoch 241/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1914439296.0000 - val_loss: 1810003840.0000\n",
      "Epoch 242/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1898136192.0000 - val_loss: 1807313536.0000\n",
      "Epoch 243/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1907429120.0000 - val_loss: 1808194304.0000\n",
      "Epoch 244/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906349184.0000 - val_loss: 1812176512.0000\n",
      "Epoch 245/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1909080192.0000 - val_loss: 1807681920.0000\n",
      "Epoch 246/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906596864.0000 - val_loss: 1817037952.0000\n",
      "Epoch 247/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893393408.0000 - val_loss: 1809574144.0000\n",
      "Epoch 248/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1913450112.0000 - val_loss: 1809634176.0000\n",
      "Epoch 249/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895956352.0000 - val_loss: 1807636352.0000\n",
      "Epoch 250/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897911936.0000 - val_loss: 1810044160.0000\n",
      "Epoch 251/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893549696.0000 - val_loss: 1807423104.0000\n",
      "Epoch 252/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899078272.0000 - val_loss: 1809499648.0000\n",
      "Epoch 253/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1911010432.0000 - val_loss: 1807640192.0000\n",
      "Epoch 254/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1897244160.0000 - val_loss: 1807267584.0000\n",
      "Epoch 255/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902386176.0000 - val_loss: 1808612608.0000\n",
      "Epoch 256/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1903729536.0000 - val_loss: 1807014784.0000\n",
      "Epoch 257/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1912885376.0000 - val_loss: 1822928512.0000\n",
      "Epoch 258/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902668544.0000 - val_loss: 1806490112.0000\n",
      "Epoch 259/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899831168.0000 - val_loss: 1806585856.0000\n",
      "Epoch 260/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1918845568.0000 - val_loss: 1807107712.0000\n",
      "Epoch 261/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1892358016.0000 - val_loss: 1807173888.0000\n",
      "Epoch 262/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896999040.0000 - val_loss: 1807030144.0000\n",
      "Epoch 263/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895580416.0000 - val_loss: 1814470400.0000\n",
      "Epoch 264/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901659264.0000 - val_loss: 1820395776.0000\n",
      "Epoch 265/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1918423936.0000 - val_loss: 1809300352.0000\n",
      "Epoch 266/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901661056.0000 - val_loss: 1806973568.0000\n",
      "Epoch 267/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895458560.0000 - val_loss: 1806650240.0000\n",
      "Epoch 268/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898544384.0000 - val_loss: 1810774016.0000\n",
      "Epoch 269/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1894614912.0000 - val_loss: 1806241792.0000\n",
      "Epoch 270/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902568832.0000 - val_loss: 1805731968.0000\n",
      "Epoch 271/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900198784.0000 - val_loss: 1805745920.0000\n",
      "Epoch 272/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900972928.0000 - val_loss: 1806076160.0000\n",
      "Epoch 273/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1909330176.0000 - val_loss: 1832377856.0000\n",
      "Epoch 274/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893937024.0000 - val_loss: 1820876160.0000\n",
      "Epoch 275/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1916292480.0000 - val_loss: 1805895936.0000\n",
      "Epoch 276/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1893797632.0000 - val_loss: 1805376256.0000\n",
      "Epoch 277/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895944320.0000 - val_loss: 1811697280.0000\n",
      "Epoch 278/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901408384.0000 - val_loss: 1813863808.0000\n",
      "Epoch 279/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906538880.0000 - val_loss: 1810240256.0000\n",
      "Epoch 280/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1894249344.0000 - val_loss: 1806645888.0000\n",
      "Epoch 281/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902987648.0000 - val_loss: 1807932160.0000\n",
      "Epoch 282/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905221504.0000 - val_loss: 1815666304.0000\n",
      "Epoch 283/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1908550272.0000 - val_loss: 1810636672.0000\n",
      "Epoch 284/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898692736.0000 - val_loss: 1805930624.0000\n",
      "Epoch 285/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1890572416.0000 - val_loss: 1805072384.0000\n",
      "Epoch 286/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894397952.0000 - val_loss: 1807802624.0000\n",
      "Epoch 287/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902051456.0000 - val_loss: 1817895296.0000\n",
      "Epoch 288/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1913278976.0000 - val_loss: 1805538304.0000\n",
      "Epoch 289/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899594752.0000 - val_loss: 1808329216.0000\n",
      "Epoch 290/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1917244288.0000 - val_loss: 1806680960.0000\n",
      "Epoch 291/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893198848.0000 - val_loss: 1805576192.0000\n",
      "Epoch 292/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1892941696.0000 - val_loss: 1813249024.0000\n",
      "Epoch 293/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1904665984.0000 - val_loss: 1804677888.0000\n",
      "Epoch 294/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897346688.0000 - val_loss: 1807990528.0000\n",
      "Epoch 295/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900754176.0000 - val_loss: 1806065024.0000\n",
      "Epoch 296/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1895497472.0000 - val_loss: 1804400256.0000\n",
      "Epoch 297/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900768128.0000 - val_loss: 1804473216.0000\n",
      "Epoch 298/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903600384.0000 - val_loss: 1813324416.0000\n",
      "Epoch 299/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1903314560.0000 - val_loss: 1804599040.0000\n",
      "Epoch 300/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901653504.0000 - val_loss: 1805870720.0000\n",
      "Epoch 301/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905993856.0000 - val_loss: 1813220352.0000\n",
      "Epoch 302/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1895033344.0000 - val_loss: 1804384000.0000\n",
      "Epoch 303/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899919744.0000 - val_loss: 1810478848.0000\n",
      "Epoch 304/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1889992448.0000 - val_loss: 1804018176.0000\n",
      "Epoch 305/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898924928.0000 - val_loss: 1811948032.0000\n",
      "Epoch 306/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897734528.0000 - val_loss: 1807244800.0000\n",
      "Epoch 307/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1916534144.0000 - val_loss: 1804792576.0000\n",
      "Epoch 308/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1910187392.0000 - val_loss: 1804030976.0000\n",
      "Epoch 309/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901812736.0000 - val_loss: 1805179520.0000\n",
      "Epoch 310/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902949632.0000 - val_loss: 1812906624.0000\n",
      "Epoch 311/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906992768.0000 - val_loss: 1804156800.0000\n",
      "Epoch 312/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898838656.0000 - val_loss: 1811750144.0000\n",
      "Epoch 313/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899414144.0000 - val_loss: 1804784512.0000\n",
      "Epoch 314/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894767744.0000 - val_loss: 1804829568.0000\n",
      "Epoch 315/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896373248.0000 - val_loss: 1810183680.0000\n",
      "Epoch 316/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906093056.0000 - val_loss: 1813682944.0000\n",
      "Epoch 317/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1929602304.0000 - val_loss: 1809800192.0000\n",
      "Epoch 318/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897626496.0000 - val_loss: 1804044544.0000\n",
      "Epoch 319/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903473280.0000 - val_loss: 1822052224.0000\n",
      "Epoch 320/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1908552064.0000 - val_loss: 1803807232.0000\n",
      "Epoch 321/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891117568.0000 - val_loss: 1806886016.0000\n",
      "Epoch 322/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897536128.0000 - val_loss: 1805247744.0000\n",
      "Epoch 323/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1901602048.0000 - val_loss: 1803543168.0000\n",
      "Epoch 324/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904995456.0000 - val_loss: 1803686656.0000\n",
      "Epoch 325/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897616512.0000 - val_loss: 1804022016.0000\n",
      "Epoch 326/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905335808.0000 - val_loss: 1807878400.0000\n",
      "Epoch 327/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1912868096.0000 - val_loss: 1805058176.0000\n",
      "Epoch 328/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891529600.0000 - val_loss: 1806148224.0000\n",
      "Epoch 329/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898099584.0000 - val_loss: 1806729472.0000\n",
      "Epoch 330/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903510784.0000 - val_loss: 1817611136.0000\n",
      "Epoch 331/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1933460480.0000 - val_loss: 1805230080.0000\n",
      "Epoch 332/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1925725056.0000 - val_loss: 1812012672.0000\n",
      "Epoch 333/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1912890752.0000 - val_loss: 1803582336.0000\n",
      "Epoch 334/2000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1893829888.0000 - val_loss: 1803412864.0000\n",
      "Epoch 335/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905824768.0000 - val_loss: 1803418624.0000\n",
      "Epoch 336/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902742144.0000 - val_loss: 1809523328.0000\n",
      "Epoch 337/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895778432.0000 - val_loss: 1805803392.0000\n",
      "Epoch 338/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902535680.0000 - val_loss: 1804297856.0000\n",
      "Epoch 339/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1910245888.0000 - val_loss: 1809328512.0000\n",
      "Epoch 340/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1907793792.0000 - val_loss: 1804447360.0000\n",
      "Epoch 341/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1893376000.0000 - val_loss: 1803081600.0000\n",
      "Epoch 342/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897733760.0000 - val_loss: 1803131136.0000\n",
      "Epoch 343/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1913289856.0000 - val_loss: 1808029824.0000\n",
      "Epoch 344/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891749248.0000 - val_loss: 1805276416.0000\n",
      "Epoch 345/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895494528.0000 - val_loss: 1803184384.0000\n",
      "Epoch 346/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1893361408.0000 - val_loss: 1803068032.0000\n",
      "Epoch 347/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1914713344.0000 - val_loss: 1824668800.0000\n",
      "Epoch 348/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1919986176.0000 - val_loss: 1812058496.0000\n",
      "Epoch 349/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896743552.0000 - val_loss: 1809633536.0000\n",
      "Epoch 350/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1901436416.0000 - val_loss: 1807589632.0000\n",
      "Epoch 351/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1898022528.0000 - val_loss: 1802775680.0000\n",
      "Epoch 352/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896389376.0000 - val_loss: 1813666688.0000\n",
      "Epoch 353/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902554880.0000 - val_loss: 1810567168.0000\n",
      "Epoch 354/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896282624.0000 - val_loss: 1804537728.0000\n",
      "Epoch 355/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891307776.0000 - val_loss: 1803062784.0000\n",
      "Epoch 356/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1925413376.0000 - val_loss: 1811937536.0000\n",
      "Epoch 357/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1912850688.0000 - val_loss: 1814223744.0000\n",
      "Epoch 358/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1899926528.0000 - val_loss: 1805852160.0000\n",
      "Epoch 359/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1892645760.0000 - val_loss: 1815744512.0000\n",
      "Epoch 360/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896940032.0000 - val_loss: 1802820224.0000\n",
      "Epoch 361/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890578048.0000 - val_loss: 1802939520.0000\n",
      "Epoch 362/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896273536.0000 - val_loss: 1804603392.0000\n",
      "Epoch 363/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896715264.0000 - val_loss: 1809106560.0000\n",
      "Epoch 364/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1912470400.0000 - val_loss: 1815637632.0000\n",
      "Epoch 365/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1911089024.0000 - val_loss: 1802405632.0000\n",
      "Epoch 366/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1893639424.0000 - val_loss: 1802214016.0000\n",
      "Epoch 367/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894528512.0000 - val_loss: 1802656384.0000\n",
      "Epoch 368/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894520960.0000 - val_loss: 1802226816.0000\n",
      "Epoch 369/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902390656.0000 - val_loss: 1802177408.0000\n",
      "Epoch 370/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896595840.0000 - val_loss: 1809836032.0000\n",
      "Epoch 371/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895780096.0000 - val_loss: 1806278400.0000\n",
      "Epoch 372/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1925450624.0000 - val_loss: 1815057280.0000\n",
      "Epoch 373/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899456640.0000 - val_loss: 1803729024.0000\n",
      "Epoch 374/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905828480.0000 - val_loss: 1802339200.0000\n",
      "Epoch 375/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894923264.0000 - val_loss: 1802420224.0000\n",
      "Epoch 376/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1909523072.0000 - val_loss: 1809435264.0000\n",
      "Epoch 377/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891431296.0000 - val_loss: 1802345088.0000\n",
      "Epoch 378/2000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1891946240.0000 - val_loss: 1802104576.0000\n",
      "Epoch 379/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902212608.0000 - val_loss: 1801438208.0000\n",
      "Epoch 380/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1914294912.0000 - val_loss: 1801647488.0000\n",
      "Epoch 381/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899907584.0000 - val_loss: 1803617408.0000\n",
      "Epoch 382/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1899643648.0000 - val_loss: 1801338880.0000\n",
      "Epoch 383/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894271360.0000 - val_loss: 1803995648.0000\n",
      "Epoch 384/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894624384.0000 - val_loss: 1802478080.0000\n",
      "Epoch 385/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896780288.0000 - val_loss: 1801603840.0000\n",
      "Epoch 386/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890451584.0000 - val_loss: 1805970688.0000\n",
      "Epoch 387/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903738752.0000 - val_loss: 1804858496.0000\n",
      "Epoch 388/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1900827648.0000 - val_loss: 1801258112.0000\n",
      "Epoch 389/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906681472.0000 - val_loss: 1819624064.0000\n",
      "Epoch 390/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897072640.0000 - val_loss: 1801628160.0000\n",
      "Epoch 391/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893093504.0000 - val_loss: 1801470208.0000\n",
      "Epoch 392/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1892414848.0000 - val_loss: 1803638016.0000\n",
      "Epoch 393/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1905601664.0000 - val_loss: 1801185280.0000\n",
      "Epoch 394/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894080768.0000 - val_loss: 1805854848.0000\n",
      "Epoch 395/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894239104.0000 - val_loss: 1801369856.0000\n",
      "Epoch 396/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890026112.0000 - val_loss: 1805500160.0000\n",
      "Epoch 397/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895374464.0000 - val_loss: 1802782848.0000\n",
      "Epoch 398/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890394240.0000 - val_loss: 1801672576.0000\n",
      "Epoch 399/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1890021120.0000 - val_loss: 1800911104.0000\n",
      "Epoch 400/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904694656.0000 - val_loss: 1804277888.0000\n",
      "Epoch 401/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903274624.0000 - val_loss: 1807867392.0000\n",
      "Epoch 402/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895208448.0000 - val_loss: 1801173376.0000\n",
      "Epoch 403/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1919147904.0000 - val_loss: 1807214720.0000\n",
      "Epoch 404/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890597248.0000 - val_loss: 1805804032.0000\n",
      "Epoch 405/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1888338688.0000 - val_loss: 1803853824.0000\n",
      "Epoch 406/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1902782080.0000 - val_loss: 1803751808.0000\n",
      "Epoch 407/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897524352.0000 - val_loss: 1813487488.0000\n",
      "Epoch 408/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891868672.0000 - val_loss: 1801214336.0000\n",
      "Epoch 409/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1892486400.0000 - val_loss: 1800471040.0000\n",
      "Epoch 410/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894766848.0000 - val_loss: 1804021760.0000\n",
      "Epoch 411/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897122688.0000 - val_loss: 1800707584.0000\n",
      "Epoch 412/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895097216.0000 - val_loss: 1800886784.0000\n",
      "Epoch 413/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891138688.0000 - val_loss: 1800717312.0000\n",
      "Epoch 414/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894077824.0000 - val_loss: 1808856832.0000\n",
      "Epoch 415/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1893225472.0000 - val_loss: 1800393856.0000\n",
      "Epoch 416/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897924480.0000 - val_loss: 1804795392.0000\n",
      "Epoch 417/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1889525888.0000 - val_loss: 1800441088.0000\n",
      "Epoch 418/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893189504.0000 - val_loss: 1800989824.0000\n",
      "Epoch 419/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1898154624.0000 - val_loss: 1800704896.0000\n",
      "Epoch 420/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1896127232.0000 - val_loss: 1800076160.0000\n",
      "Epoch 421/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1892772352.0000 - val_loss: 1801612288.0000\n",
      "Epoch 422/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1913888512.0000 - val_loss: 1817453056.0000\n",
      "Epoch 423/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904932352.0000 - val_loss: 1801577600.0000\n",
      "Epoch 424/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894580864.0000 - val_loss: 1805821440.0000\n",
      "Epoch 425/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901853056.0000 - val_loss: 1818765568.0000\n",
      "Epoch 426/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1907317888.0000 - val_loss: 1802577664.0000\n",
      "Epoch 427/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893372928.0000 - val_loss: 1806630784.0000\n",
      "Epoch 428/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1925476608.0000 - val_loss: 1804882048.0000\n",
      "Epoch 429/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1899900800.0000 - val_loss: 1801501952.0000\n",
      "Epoch 430/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1893064576.0000 - val_loss: 1799686528.0000\n",
      "Epoch 431/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1897216768.0000 - val_loss: 1799560576.0000\n",
      "Epoch 432/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1893049856.0000 - val_loss: 1803918464.0000\n",
      "Epoch 433/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1889321344.0000 - val_loss: 1799376000.0000\n",
      "Epoch 434/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896823808.0000 - val_loss: 1805023872.0000\n",
      "Epoch 435/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898121984.0000 - val_loss: 1806051840.0000\n",
      "Epoch 436/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901973248.0000 - val_loss: 1801803264.0000\n",
      "Epoch 437/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1915713792.0000 - val_loss: 1809771264.0000\n",
      "Epoch 438/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890400896.0000 - val_loss: 1801177600.0000\n",
      "Epoch 439/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893040384.0000 - val_loss: 1800846848.0000\n",
      "Epoch 440/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1897863424.0000 - val_loss: 1799325440.0000\n",
      "Epoch 441/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1892904064.0000 - val_loss: 1799811456.0000\n",
      "Epoch 442/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1901289600.0000 - val_loss: 1801450496.0000\n",
      "Epoch 443/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903918080.0000 - val_loss: 1804131968.0000\n",
      "Epoch 444/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1894088576.0000 - val_loss: 1799883392.0000\n",
      "Epoch 445/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1898583424.0000 - val_loss: 1803626240.0000\n",
      "Epoch 446/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895681024.0000 - val_loss: 1799926784.0000\n",
      "Epoch 447/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894039296.0000 - val_loss: 1802782080.0000\n",
      "Epoch 448/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1907121664.0000 - val_loss: 1799414784.0000\n",
      "Epoch 449/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1904916224.0000 - val_loss: 1812113664.0000\n",
      "Epoch 450/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1908546432.0000 - val_loss: 1807334144.0000\n",
      "Epoch 451/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895023232.0000 - val_loss: 1799765888.0000\n",
      "Epoch 452/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1894184576.0000 - val_loss: 1798990848.0000\n",
      "Epoch 453/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893101056.0000 - val_loss: 1799032832.0000\n",
      "Epoch 454/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897565824.0000 - val_loss: 1800869504.0000\n",
      "Epoch 455/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1896473856.0000 - val_loss: 1805239168.0000\n",
      "Epoch 456/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897784320.0000 - val_loss: 1799374336.0000\n",
      "Epoch 457/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1891897856.0000 - val_loss: 1801460864.0000\n",
      "Epoch 458/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890710272.0000 - val_loss: 1800714112.0000\n",
      "Epoch 459/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1925403648.0000 - val_loss: 1805384064.0000\n",
      "Epoch 460/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1893186816.0000 - val_loss: 1798398592.0000\n",
      "Epoch 461/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1896542464.0000 - val_loss: 1800634240.0000\n",
      "Epoch 462/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1886764672.0000 - val_loss: 1799229696.0000\n",
      "Epoch 463/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890034432.0000 - val_loss: 1798504960.0000\n",
      "Epoch 464/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1890000384.0000 - val_loss: 1798145408.0000\n",
      "Epoch 465/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893853696.0000 - val_loss: 1798732416.0000\n",
      "Epoch 466/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1897751296.0000 - val_loss: 1799160192.0000\n",
      "Epoch 467/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1913361920.0000 - val_loss: 1803401856.0000\n",
      "Epoch 468/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894209280.0000 - val_loss: 1802291200.0000\n",
      "Epoch 469/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1889153792.0000 - val_loss: 1799394688.0000\n",
      "Epoch 470/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1887756672.0000 - val_loss: 1798175616.0000\n",
      "Epoch 471/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1892220160.0000 - val_loss: 1804323456.0000\n",
      "Epoch 472/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893816448.0000 - val_loss: 1798959744.0000\n",
      "Epoch 473/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1889406208.0000 - val_loss: 1799569024.0000\n",
      "Epoch 474/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890937600.0000 - val_loss: 1799457664.0000\n",
      "Epoch 475/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1887664128.0000 - val_loss: 1797868672.0000\n",
      "Epoch 476/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895771008.0000 - val_loss: 1799752064.0000\n",
      "Epoch 477/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1907705856.0000 - val_loss: 1800498304.0000\n",
      "Epoch 478/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905606016.0000 - val_loss: 1798009856.0000\n",
      "Epoch 479/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1915856384.0000 - val_loss: 1806013696.0000\n",
      "Epoch 480/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1916843136.0000 - val_loss: 1816833536.0000\n",
      "Epoch 481/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1892231296.0000 - val_loss: 1811893376.0000\n",
      "Epoch 482/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900204032.0000 - val_loss: 1798818944.0000\n",
      "Epoch 483/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1894451072.0000 - val_loss: 1797798784.0000\n",
      "Epoch 484/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894697728.0000 - val_loss: 1799227264.0000\n",
      "Epoch 485/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1895780864.0000 - val_loss: 1802396032.0000\n",
      "Epoch 486/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1894153984.0000 - val_loss: 1800631168.0000\n",
      "Epoch 487/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1888263040.0000 - val_loss: 1798032128.0000\n",
      "Epoch 488/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1892748032.0000 - val_loss: 1798290944.0000\n",
      "Epoch 489/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1900596864.0000 - val_loss: 1808435712.0000\n",
      "Epoch 490/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890124288.0000 - val_loss: 1809961088.0000\n",
      "Epoch 491/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905834752.0000 - val_loss: 1802764288.0000\n",
      "Epoch 492/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1902296960.0000 - val_loss: 1805794944.0000\n",
      "Epoch 493/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1903250304.0000 - val_loss: 1804986880.0000\n",
      "Epoch 494/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1885529216.0000 - val_loss: 1801744000.0000\n",
      "Epoch 495/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1893283840.0000 - val_loss: 1798002432.0000\n",
      "Epoch 496/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1897870080.0000 - val_loss: 1804008448.0000\n",
      "Epoch 497/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1894233856.0000 - val_loss: 1798896256.0000\n",
      "Epoch 498/2000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1898657664.0000 - val_loss: 1805754496.0000\n",
      "Epoch 499/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1910235648.0000 - val_loss: 1797916672.0000\n",
      "Epoch 500/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1905473152.0000 - val_loss: 1800736256.0000\n",
      "Epoch 501/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1906881536.0000 - val_loss: 1808314880.0000\n",
      "Epoch 502/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1888325120.0000 - val_loss: 1802150016.0000\n",
      "Epoch 503/2000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1890185088.0000 - val_loss: 1798976640.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#데이터를 불러옴\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/SoongMoo/soldesk20231218/main/data/house_train.csv')\n",
    "df\n",
    "\n",
    "#NaN확인\n",
    "df.isnull().sum().sort_values(ascending=False).head(20)\n",
    "                                #오름차순 : ascending\n",
    "                                #내림차순 : descending\n",
    "#원-핫 인코딩\n",
    "#카테고리형 변수를 0과 1로 이루어진 변수\n",
    "df = pd.get_dummies(df)\n",
    "#결측지를 전체 칼럼의 평균으로 대체\n",
    "df = df.fillna(df.mean())\n",
    "#df.mean()\n",
    "df\n",
    "\n",
    "#속성과 클래스로 분리\n",
    "cols_train = ['OverallQual', 'GrLivArea','GarageCars','GarageArea','TotalBsmtSF']\n",
    "X_train_pre = df[cols_train]\n",
    "y = df['SalePrice'].values\n",
    "\n",
    "#학습셋과 테스트셋\n",
    "X_train, X_test, y_train, t_test = train_test_split(X_train_pre, y, test_size = 0.2)\n",
    "\n",
    "\n",
    "#모델의 구조를 설정\n",
    "#노드의 수를 늘리면\n",
    "#-모델은 더 다양하고 복잡한 특징과 패턴을 학습할수 있음\n",
    "#-노드 수를 무작정 늘리면 과적합의 위험이 증가할 수 있음\n",
    "#-노드 수가 많을 수록 모델의 계산 비용이 증가함\n",
    "#-데이터의 특성, 문제의 복잡성, 사용가능한 훈련 데이터 양등을 고려하여 노드 수를 조절해야함\n",
    "#-노드 수를 정하는 방법\n",
    "#-교차검증, 그리드서치, 경험적방법\n",
    "#-자동화된 하이퍼 파라미터 최적화도구 사용\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim= X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#모델 실행\n",
    "model.compile(optimizer='adam', loss = 'mean_squared_error')\n",
    "#20회 이상 결과가 향상되지 않으면 자동으로 중단\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 20)\n",
    "#모델의 이름을 정하기\n",
    "modelpath='./data/model/house.hdf5'\n",
    "#최적화 모델을 업데이트 하고 저장\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor = 'val_loss', verbose=0, save_best_only = True)\n",
    "#검증셋(validation_split) : 25%\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size=32, validation_split= 0.25, \n",
    "                    callbacks=[early_stopping_callback, checkpointer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0c5e25a-5b28-4637-9940-119d5c2ddda0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1893791973.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[50], line 17\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f'실제 가격: {real:.2f}, 예측값: {prediction[0].2f}')\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "real_prices = []  # 시각화를 위해 실제값 저장\n",
    "pred_prices = []  # 시각화를 위해 예측값 저장\n",
    "X_num = []\n",
    "\n",
    "# SalePrice의 예측값\n",
    "y_prediction = model.predict(X_test)\n",
    "y_test\n",
    "y_prediction\n",
    "n_iter = 0\n",
    "\n",
    "# 25개의 샘플에 대해서 시각화\n",
    "for i in range(min(25, len(y_test))):\n",
    "    real = y_test[i]\n",
    "    prediction = y_prediction[i]\n",
    "    \n",
    "    # 형식 지정자를 사용하지 않고 출력\n",
    "    print(f'실제 가격: {real:.2f}, 예측값: {prediction[0].2f}')\n",
    "    \n",
    "    real_prices.append(real)\n",
    "    pred_prices.append(prediction)\n",
    "    X_num.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d8895e9-0a15-4a49-b7a6-07ae75168916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(X_num, pred_prices, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted prices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreal prices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:3575\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3567\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3569\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1723\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1721\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scalex:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_autoscale_view(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:2309\u001b[0m, in \u001b[0;36m_AxesBase.add_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2307\u001b[0m     line\u001b[38;5;241m.\u001b[39mset_clip_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch)\n\u001b[1;32m-> 2309\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_line_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mget_label():\n\u001b[0;32m   2311\u001b[0m     line\u001b[38;5;241m.\u001b[39mset_label(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_child\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:2332\u001b[0m, in \u001b[0;36m_AxesBase._update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_line_limits\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2330\u001b[0m \u001b[38;5;124;03m    Figures out the data limit of the given line, updating self.dataLim.\u001b[39;00m\n\u001b[0;32m   2331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2332\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2334\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\lines.py:1032\u001b[0m, in \u001b[0;36mLine2D.get_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidx:\n\u001b[1;32m-> 1032\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\lines.py:674\u001b[0m, in \u001b[0;36mLine2D.recache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m always \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidy:\n\u001b[0;32m    673\u001b[0m     yconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_yorig)\n\u001b[1;32m--> 674\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_to_unmasked_float_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43myconv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    676\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\cbook.py:1345\u001b[0m, in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39masarray(x, \u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mfilled(np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(x, \u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ellipsis'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X_num, pred_prices, label = 'predicted prices')\n",
    "plt.plot(X_num, real_prices, label = 'real prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0375c09b-d7b8-4372-a1ba-261e8d01944e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20103ce4a90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "#MNIST 데이터셋을 불러와 학습셋과 테스트셋으로 저장\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "X_train.shape[0] #60000\n",
    "X_test.shape[0] #10000\n",
    "plt.imshow(X_train[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc967e6f-bf57-47dc-8fbd-bb450eab4b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  3  18 18 18 12613617526 1662552471270  0  0  0  \n",
      "0  0  0  0  0  0  0  0  30 36 94 15417025325325325325322517225324219564 0  0  0  0  \n",
      "0  0  0  0  0  0  0  49 23825325325325325325325325325193 82 82 56 39 0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  18 2192532532532532531981822472410  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  80 15610725325320511 0  43 1540  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  14 1  15425390 0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  1392531902  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  11 19025370 0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  35 2412251601081  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  81 24025325311925 0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  45 18625325315027 0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  16 93 2522531870  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  24925324964 0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  46 1301832532532072  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  39 1482292532532532501820  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  24 11422125325325325320178 0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  23 66 21325325325325319881 2  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  18 17121925325325325319580 9  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  55 17222625325325325324413311 0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  13625325325321213513216 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for x in X_train[0]:\n",
    "    for i in x:\n",
    "        sys.stdout.write('%-3s' % i)\n",
    "    sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9fb2fdfc-ec90-4ffe-9f50-799cb4cb4c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "X_train #2차원\n",
    "#차원변환\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_train.shape #(60000, 784) 1차원됨\n",
    "X_train = X_train.astype('float64')\n",
    "X_train = X_train / 255\n",
    "X_test = X_train.reshape(X_train.shape[0], 784).astype('float64') / 255\n",
    "\n",
    "#속성 : X_train[0]\n",
    "#class : y_train[0]\n",
    "y_train[0]\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea951d03-7db3-461d-abf3-480892b1da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Temp\\ipykernel_11812\\3298472973.py\", line 24, in <module>\n",
      "    history = model.fit(X_train, y_train, validation_split= 0.25, epochs=30 \\\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "    filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Model.fit() got an unexpected keyword argument 'callback'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1160, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 283, in executing\n",
      "    assert_(new_stmts <= stmts)\n",
      "  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 80, in assert_\n",
      "    raise AssertionError(str(message))\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "#데이터 불러오기\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#모델 구조를 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#모델 실행\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#모델최적화\n",
    "model.path='./data/model/MNIST_MLP.hdf5'\n",
    "early_stopping_callback = EarlyStopping(monitor='var_loss', patience=10)\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0,save_best_only=True)\n",
    "\n",
    "#모델학습\n",
    "history = model.fit(X_train, y_train, validation_split= 0.25, epochs=30 \\\n",
    "         , batch_size=200, verbose=1, callback=[early_stopping_callback, checkpointer])\n",
    "\n",
    "#테스트정확도\n",
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9513bf74-63ac-44dd-b1af-8652d8d0471c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_val_loss = history.history['val_loss'] #테스트셋\n",
    "y_loss = history.history['loss'] #학습셋\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_val_loss, marker = '.', c = 'red', label = 'Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker = '.', c = 'blue', label = 'Trainset_loss')\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58d9aced-ee8c-4101-8fca-3b61f3f77fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['해보지', '않으면', '해낼수', '없다']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#텍스트 전처리와 관련한 함수\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#전처리할 텍스트\n",
    "text='해보지 않으면 해낼수 없다'\n",
    "#해당 텍스트를 토큰화\n",
    "result = text_to_word_sequence(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "123737ef-8bd3-41e6-8b2c-ae19fa25873f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'텍스트의': 1,\n",
       " '딥러닝에서': 2,\n",
       " '먼저': 3,\n",
       " '각': 4,\n",
       " '단어들을': 5,\n",
       " '나누어': 6,\n",
       " '토큰화': 7,\n",
       " '한다': 8,\n",
       " '단어로': 9,\n",
       " '토큰화해야': 10,\n",
       " '인식됨': 11,\n",
       " '토큰화한': 12,\n",
       " '결과는': 13,\n",
       " '사용할수': 14,\n",
       " '있음': 15}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#전처리하려는 세개의 문장을 정함\n",
    "docs = ['먼저 텍스트의 각 단어들을 나누어 토큰화 한다',\n",
    "        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됨',\n",
    "        '토큰화한 결과는 딥러닝에서 사용할수 있음']\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "#빈도수를 확인 \n",
    "token.word_counts\n",
    "#문장의 수\n",
    "token.document_count\n",
    "#각단어의 index\n",
    "token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5618c9a0-2cec-4a69-909b-2a05253e004f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '먼저 텍스트의 각 단어들을 나누어 토큰화 한다'\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([text])\n",
    "token.word_index\n",
    "token.word_index['먼저'] #단어가 key가 된걸 확인가능\n",
    "x = token.texts_to_sequences([text])\n",
    "x\n",
    "\n",
    "x = to_categorical(x,len(token.word_index) + 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "35c6b305-b7a1-492d-b0ac-caf6b0c78678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'영화': 1, '개': 2, '꿀잼': 3, '최고임': 4, '잘': 5, '만든': 6, '추천하고': 7, '싶은': 8, '한번': 9, '더': 10, '보고싶음': 11, '글쎄': 12, '별로': 13, '생각보다': 14, '지루': 15, '연기가': 16, '어색함': 17, '노잼': 18}\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 0.7039 - accuracy: 0.3000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7020 - accuracy: 0.3000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7001 - accuracy: 0.3000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.3000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6964 - accuracy: 0.3000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6945 - accuracy: 0.3000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6927 - accuracy: 0.3000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.6000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6871 - accuracy: 0.6000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.6000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.7000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6816 - accuracy: 0.8000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6797 - accuracy: 0.8000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6778 - accuracy: 0.8000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6759 - accuracy: 0.9000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6741 - accuracy: 0.9000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6722 - accuracy: 0.9000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6703 - accuracy: 0.9000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6684 - accuracy: 0.9000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6665 - accuracy: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6587 - accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6567 - accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6547 - accuracy: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6527 - accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6507 - accuracy: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6487 - accuracy: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6466 - accuracy: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6425 - accuracy: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6383 - accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6361 - accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6340 - accuracy: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6318 - accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6296 - accuracy: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6274 - accuracy: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6252 - accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6230 - accuracy: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6207 - accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6184 - accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6161 - accuracy: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6138 - accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6115 - accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6091 - accuracy: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6067 - accuracy: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6044 - accuracy: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5995 - accuracy: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5971 - accuracy: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5946 - accuracy: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5921 - accuracy: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5896 - accuracy: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5871 - accuracy: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5846 - accuracy: 1.0000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5820 - accuracy: 1.0000\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5794 - accuracy: 1.0000\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5768 - accuracy: 1.0000\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5742 - accuracy: 1.0000\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5716 - accuracy: 1.0000\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5690 - accuracy: 1.0000\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5663 - accuracy: 1.0000\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5637 - accuracy: 1.0000\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5610 - accuracy: 1.0000\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5583 - accuracy: 1.0000\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5556 - accuracy: 1.0000\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5528 - accuracy: 1.0000\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5501 - accuracy: 1.0000\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5474 - accuracy: 1.0000\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5446 - accuracy: 1.0000\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5418 - accuracy: 1.0000\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5390 - accuracy: 1.0000\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5362 - accuracy: 1.0000\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5334 - accuracy: 1.0000\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5306 - accuracy: 1.0000\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5278 - accuracy: 1.0000\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5249 - accuracy: 1.0000\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5221 - accuracy: 1.0000\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5192 - accuracy: 1.0000\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5164 - accuracy: 1.0000\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5135 - accuracy: 1.0000\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5106 - accuracy: 1.0000\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 1.0000\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5048 - accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 1.0000\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4990 - accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4961 - accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4932 - accuracy: 1.0000\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4903 - accuracy: 1.0000\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4874 - accuracy: 1.0000\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4845 - accuracy: 1.0000\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4815 - accuracy: 1.0000\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4786 - accuracy: 1.0000\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4757 - accuracy: 1.0000\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4728 - accuracy: 1.0000\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4698 - accuracy: 1.0000\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4669 - accuracy: 1.0000\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4640 - accuracy: 1.0000\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4611 - accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 1.0000\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 1.0000\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 1.0000\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4494 - accuracy: 1.0000\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4465 - accuracy: 1.0000\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4436 - accuracy: 1.0000\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4407 - accuracy: 1.0000\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 1.0000\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 1.0000\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4262 - accuracy: 1.0000\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4234 - accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4205 - accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4177 - accuracy: 1.0000\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4148 - accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 1.0000\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4092 - accuracy: 1.0000\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4063 - accuracy: 1.0000\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4035 - accuracy: 1.0000\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4007 - accuracy: 1.0000\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3979 - accuracy: 1.0000\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3951 - accuracy: 1.0000\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3924 - accuracy: 1.0000\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3896 - accuracy: 1.0000\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 1.0000\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 1.0000\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3814 - accuracy: 1.0000\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3787 - accuracy: 1.0000\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3733 - accuracy: 1.0000\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 1.0000\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 1.0000\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3653 - accuracy: 1.0000\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 1.0000\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3601 - accuracy: 1.0000\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 1.0000\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 1.0000\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3523 - accuracy: 1.0000\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 1.0000\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3471 - accuracy: 1.0000\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3446 - accuracy: 1.0000\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3421 - accuracy: 1.0000\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3396 - accuracy: 1.0000\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3371 - accuracy: 1.0000\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3346 - accuracy: 1.0000\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 1.0000\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 1.0000\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3272 - accuracy: 1.0000\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 1.0000\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 1.0000\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 1.0000\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 1.0000\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 1.0000\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3130 - accuracy: 1.0000\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3106 - accuracy: 1.0000\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3083 - accuracy: 1.0000\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3060 - accuracy: 1.0000\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 1.0000\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3015 - accuracy: 1.0000\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 1.0000\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2970 - accuracy: 1.0000\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 1.0000\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2926 - accuracy: 1.0000\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2904 - accuracy: 1.0000\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2883 - accuracy: 1.0000\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2861 - accuracy: 1.0000\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2840 - accuracy: 1.0000\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2819 - accuracy: 1.0000\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2798 - accuracy: 1.0000\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2777 - accuracy: 1.0000\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2756 - accuracy: 1.0000\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2736 - accuracy: 1.0000\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2715 - accuracy: 1.0000\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2695 - accuracy: 1.0000\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2675 - accuracy: 1.0000\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2655 - accuracy: 1.0000\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2636 - accuracy: 1.0000\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2616 - accuracy: 1.0000\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2597 - accuracy: 1.0000\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2577 - accuracy: 1.0000\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2558 - accuracy: 1.0000\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2539 - accuracy: 1.0000\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 1.0000\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2502 - accuracy: 1.0000\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2483 - accuracy: 1.0000\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2465 - accuracy: 1.0000\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2447 - accuracy: 1.0000\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2429 - accuracy: 1.0000\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2411 - accuracy: 1.0000\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2393 - accuracy: 1.0000\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2375 - accuracy: 1.0000\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2358 - accuracy: 1.0000\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2340 - accuracy: 1.0000\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 1.0000\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2306 - accuracy: 1.0000\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2289 - accuracy: 1.0000\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 1.0000\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 1.0000\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 1.0000\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2223 - accuracy: 1.0000\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2207 - accuracy: 1.0000\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2191 - accuracy: 1.0000\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2175 - accuracy: 1.0000\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 1.0000\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2143 - accuracy: 1.0000\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2128 - accuracy: 1.0000\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2112 - accuracy: 1.0000\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 1.0000\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 1.0000\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 1.0000\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 1.0000\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 1.0000\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2023 - accuracy: 1.0000\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2008 - accuracy: 1.0000\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1994 - accuracy: 1.0000\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 1.0000\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 1.0000\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1951 - accuracy: 1.0000\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1937 - accuracy: 1.0000\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1923 - accuracy: 1.0000\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1910 - accuracy: 1.0000\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1896 - accuracy: 1.0000\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 1.0000\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1869 - accuracy: 1.0000\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 1.0000\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1843 - accuracy: 1.0000\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1830 - accuracy: 1.0000\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 1.0000\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1804 - accuracy: 1.0000\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1792 - accuracy: 1.0000\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 1.0000\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1767 - accuracy: 1.0000\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1754 - accuracy: 1.0000\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 1.0000\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 1.0000\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 1.0000\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1706 - accuracy: 1.0000\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1694 - accuracy: 1.0000\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1682 - accuracy: 1.0000\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1670 - accuracy: 1.0000\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 1.0000\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1647 - accuracy: 1.0000\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1636 - accuracy: 1.0000\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1625 - accuracy: 1.0000\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 1.0000\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 1.0000\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 1.0000\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1581 - accuracy: 1.0000\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1570 - accuracy: 1.0000\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1559 - accuracy: 1.0000\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1548 - accuracy: 1.0000\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1538 - accuracy: 1.0000\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 1.0000\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 1.0000\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1507 - accuracy: 1.0000\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 1.0000\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1486 - accuracy: 1.0000\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1476 - accuracy: 1.0000\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1466 - accuracy: 1.0000\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1457 - accuracy: 1.0000\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 1.0000\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1437 - accuracy: 1.0000\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1427 - accuracy: 1.0000\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 1.0000\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1408 - accuracy: 1.0000\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 1.0000\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1390 - accuracy: 1.0000\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 1.0000\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1371 - accuracy: 1.0000\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 1.0000\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 1.0000\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 1.0000\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 1.0000\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1326 - accuracy: 1.0000\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 1.0000\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 1.0000\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1300 - accuracy: 1.0000\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1292 - accuracy: 1.0000\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 1.0000\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 1.0000\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1267 - accuracy: 1.0000\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1258 - accuracy: 1.0000\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 1.0000\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1242 - accuracy: 1.0000\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1234 - accuracy: 1.0000\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1226 - accuracy: 1.0000\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1218 - accuracy: 1.0000\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 1.0000\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 1.0000\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 1.0000\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1187 - accuracy: 1.0000\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 1.0000\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1172 - accuracy: 1.0000\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 1.0000\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1157 - accuracy: 1.0000\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1149 - accuracy: 1.0000\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1142 - accuracy: 1.0000\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 1.0000\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 1.0000\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1120 - accuracy: 1.0000\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 1.0000\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1106 - accuracy: 1.0000\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 1.0000\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 1.0000\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1086 - accuracy: 1.0000\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1079 - accuracy: 1.0000\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 1.0000\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 1.0000\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 1.0000\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 1.0000\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1045 - accuracy: 1.0000\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 1.0000\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1032 - accuracy: 1.0000\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 1.0000\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1020 - accuracy: 1.0000\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 1.0000\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 1.0000\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1001 - accuracy: 1.0000\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 1.0000\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 1.0000\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0982 - accuracy: 1.0000\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 1.0000\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 1.0000\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0964 - accuracy: 1.0000\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 1.0000\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 1.0000\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 1.0000\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0941 - accuracy: 1.0000\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 1.0000\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0930 - accuracy: 1.0000\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 1.0000\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 1.0000\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0913 - accuracy: 1.0000\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0907 - accuracy: 1.0000\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0902 - accuracy: 1.0000\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 1.0000\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 1.0000\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 1.0000\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 1.0000\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 1.0000\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 1.0000\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 1.0000\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 1.0000\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 1.0000\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 1.0000\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 1.0000\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0839 - accuracy: 1.0000\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 1.0000\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0830 - accuracy: 1.0000\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 1.0000\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 1.0000\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 1.0000\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 1.0000\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 1.0000\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 1.0000\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0796 - accuracy: 1.0000\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 1.0000\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0787 - accuracy: 1.0000\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 1.0000\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0778 - accuracy: 1.0000\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 1.0000\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 1.0000\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 1.0000\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 1.0000\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 1.0000\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 1.0000\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 1.0000\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 1.0000\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 1.0000\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 1.0000\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 1.0000\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 1.0000\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0722 - accuracy: 1.0000\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 1.0000\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 1.0000\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0710 - accuracy: 1.0000\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 1.0000\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 1.0000\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 1.0000\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 1.0000\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 1.0000\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0686 - accuracy: 1.0000\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 1.0000\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 1.0000\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 1.0000\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 1.0000\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 1.0000\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 1.0000\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 1.0000\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 1.0000\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 1.0000\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 1.0000\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 1.0000\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 1.0000\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 1.0000\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0635 - accuracy: 1.0000\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 1.0000\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 1.0000\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 1.0000\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 1.0000\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 1.0000\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 1.0000\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 1.0000\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 1.0000\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 1.0000\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 1.0000\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 1.0000\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 1.0000\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 1.0000\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 1.0000\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 1.0000\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 1.0000\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 1.0000\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 1.0000\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 1.0000\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 1.0000\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 1.0000\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0564 - accuracy: 1.0000\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0561 - accuracy: 1.0000\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0558 - accuracy: 1.0000\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0555 - accuracy: 1.0000\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 1.0000\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 1.0000\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 1.0000\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 1.0000\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 1.0000\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0535 - accuracy: 1.0000\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0532 - accuracy: 1.0000\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 1.0000\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 1.0000\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0524 - accuracy: 1.0000\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 1.0000\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0519 - accuracy: 1.0000\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0516 - accuracy: 1.0000\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0514 - accuracy: 1.0000\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0511 - accuracy: 1.0000\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 1.0000\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 1.0000\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 1.0000\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 1.0000\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0498 - accuracy: 1.0000\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 1.0000\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 1.0000\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0491 - accuracy: 1.0000\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0488 - accuracy: 1.0000\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 1.0000\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 1.0000\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0481 - accuracy: 1.0000\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 1.0000\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 1.0000\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0474 - accuracy: 1.0000\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 1.0000\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 1.0000\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 1.0000\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 1.0000\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0462 - accuracy: 1.0000\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 1.0000\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 1.0000\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0455 - accuracy: 1.0000\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 1.0000\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0451 - accuracy: 1.0000\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0449 - accuracy: 1.0000\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 1.0000\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0444 - accuracy: 1.0000\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0442 - accuracy: 1.0000\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 1.0000\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 1.0000\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 1.0000\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 1.0000\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 1.0000\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0417 - accuracy: 1.0000\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 1.0000\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 1.0000\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 1.0000\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 1.0000\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0406 - accuracy: 1.0000\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 1.0000\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 1.0000\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 1.0000\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 1.0000\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 1.0000\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 1.0000\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 1.0000\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0370 - accuracy: 1.0000\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 1.0000\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 1.0000\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0352 - accuracy: 1.0000\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 1.0000\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0339 - accuracy: 1.0000\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 1.0000\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 1.0000\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 1.0000\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 1.0000\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 1.0000\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0253 - accuracy: 1.0000\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2010f6dd9d0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텍스트 리뷰 자료\n",
    "docs = ['개 꿀잼','최고임','잘 만든 영화','추천하고 싶은 영화',\n",
    "        '한번 더 보고싶음','글쎄','별로','생각보다 지루','연기가 어색함', '노잼']\n",
    "\n",
    "#긍정리뷰는 1, 부정리뷰는 0\n",
    "classes = array([1,1,1,1,1,0,0,0,0,0])\n",
    "#토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "token.word_index\n",
    "#index만 출력\n",
    "print(token.word_index)\n",
    "x = token.texts_to_sequences(docs)\n",
    "x\n",
    "\n",
    "#패딩, 서로 다른 길이의 데이터를 4로 맞춰줌 : pad_sequences\n",
    "padded_x = pad_sequences(x,4)\n",
    "padded_x\n",
    "\n",
    "#임베딩\n",
    "#입력될 단어의 수\n",
    "word_size = len(token.word_index) + 1\n",
    "#단어 임베딩을 포함하여 딥러닝 모델을 만듬\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size,8,input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#모델실행\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_x,classes, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0223ef56-0ef1-4bb2-a5e1-ba190a2fbbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "[[0.47864288]]\n",
      "너무 재밋네요는 부정\n"
     ]
    }
   ],
   "source": [
    "text = '너무 재밋네요'\n",
    "x_text = token.texts_to_sequences([text])\n",
    "padded_x_text = pad_sequences(x_text,4)\n",
    "\n",
    "#예측\n",
    "result = model.predict(padded_x_text)\n",
    "print(result)\n",
    "if result[0][0] >= 0.5:\n",
    "    print(f'{text}는 긍정')\n",
    "else:\n",
    "    print(f'{text}는 부정')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
